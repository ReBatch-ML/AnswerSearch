{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register models on Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.azureml_functions import get_ws\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "ws = get_ws()\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_encoder.save('models/bi_encoder')\n",
    "cross_encoder.save('models/cross_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model untrained_bi_encoder\n",
      "Registering model untrained_cross_encoder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='SemanticSearch_TRAIN', subscription_id='9da3a5d6-6bf3-4b2c-8219-88caf39f718d', resource_group='Semantic_Search'), name=untrained_cross_encoder, id=untrained_cross_encoder:1, version=1, tags={}, properties={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "Model.register(workspace=ws, model_name=\"untrained_bi_encoder\", model_path='models/bi_encoder')\n",
    "Model.register(workspace=ws, model_name=\"untrained_cross_encoder\", model_path='models/cross_encoder')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a example dataset of context embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import lorem \n",
    "import os\n",
    "\n",
    "corpus_size = 1000000\n",
    "embeddings_size = 384\n",
    "\n",
    "step_size = 1\n",
    "with open('mock_data.jsonl', 'w') as f:\n",
    "    for size in range(0, 1000000, step_size):\n",
    "        df = {}\n",
    "        fake_embeddings = np.random.randn(step_size, embeddings_size)\n",
    "        fake_embeddings = np.float32(fake_embeddings)\n",
    "        df['embedding'] = fake_embeddings.tolist()\n",
    "        df['_id'] = [\"file/path_\"+str(size + i) for i in range(step_size)]\n",
    "        df['text'] = [lorem.paragraph() for i in range(step_size)]\n",
    "\n",
    "        f.write(json.dumps(df) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['embedding', '_id', 'text'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default-f549c43b6f62b38c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/vscode/.cache/huggingface/datasets/json/default-f549c43b6f62b38c/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2686.93it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 106.96it/s]\n",
      "                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/vscode/.cache/huggingface/datasets/json/default-f549c43b6f62b38c/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['embedding', '_id', 'text'],\n",
      "    num_rows: 1000000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# embeddings_ds = Dataset.from_pandas(df)\n",
    "from datasets import load_dataset\n",
    "embeddings_ds = load_dataset(\"json\", data_files=[\"mock_data.jsonl\"])\n",
    "embeddings_ds = embeddings_ds['train']\n",
    "embeddings_ds.save_to_disk(\"mock_dataset\")\n",
    "print(embeddings_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [03:47<00:00, 4392.66ex/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "def make_numpy(example):\n",
    "    example['embedding'] = np.array(example['embedding'], dtype=np.float32)\n",
    "    return example\n",
    "\n",
    "embeddings_ds = load_from_disk('mock_dataset')\n",
    "embeddings_ds = embeddings_ds.map(make_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embeddings_ds[\"embedding\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "did not recognize array type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/SemanticSearch/notebooks/setup_.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f727562656e642f446f63756d656e74732f53656d616e746963536561726368/workspaces/SemanticSearch/notebooks/setup_.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m string_factory \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIVF\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(\u001b[39m4\u001b[39m\u001b[39m*\u001b[39msqrt(corpus_size))\u001b[39m}\u001b[39;00m\u001b[39m,Flat\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f727562656e642f446f63756d656e74732f53656d616e746963536561726368/workspaces/SemanticSearch/notebooks/setup_.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# string_factory = f\"IVF65536_HNSW32{int(4*sqrt(corpus_size))},Flat\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f727562656e642f446f63756d656e74732f53656d616e746963536561726368/workspaces/SemanticSearch/notebooks/setup_.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m embeddings_ds\u001b[39m.\u001b[39;49madd_faiss_index(column\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39membedding\u001b[39;49m\u001b[39m'\u001b[39;49m, string_factory\u001b[39m=\u001b[39;49mstring_factory, train_size\u001b[39m=\u001b[39;49mcorpus_size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py:4465\u001b[0m, in \u001b[0;36mDataset.add_faiss_index\u001b[0;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose, dtype)\u001b[0m\n\u001b[1;32m   4415\u001b[0m \u001b[39m\"\"\"Add a dense index using Faiss for fast retrieval.\u001b[39;00m\n\u001b[1;32m   4416\u001b[0m \u001b[39mBy default the index is done over the vectors of the specified column.\u001b[39;00m\n\u001b[1;32m   4417\u001b[0m \u001b[39mYou can specify :obj:`device` if you want to run it on GPU (:obj:`device` must be the GPU index).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4462\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[1;32m   4463\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4464\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformatted_as(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, columns\u001b[39m=\u001b[39m[column], dtype\u001b[39m=\u001b[39mdtype):\n\u001b[0;32m-> 4465\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madd_faiss_index(\n\u001b[1;32m   4466\u001b[0m         column\u001b[39m=\u001b[39;49mcolumn,\n\u001b[1;32m   4467\u001b[0m         index_name\u001b[39m=\u001b[39;49mindex_name,\n\u001b[1;32m   4468\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m   4469\u001b[0m         string_factory\u001b[39m=\u001b[39;49mstring_factory,\n\u001b[1;32m   4470\u001b[0m         metric_type\u001b[39m=\u001b[39;49mmetric_type,\n\u001b[1;32m   4471\u001b[0m         custom_index\u001b[39m=\u001b[39;49mcustom_index,\n\u001b[1;32m   4472\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   4473\u001b[0m         train_size\u001b[39m=\u001b[39;49mtrain_size,\n\u001b[1;32m   4474\u001b[0m         faiss_verbose\u001b[39m=\u001b[39;49mfaiss_verbose,\n\u001b[1;32m   4475\u001b[0m     )\n\u001b[1;32m   4476\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/datasets/search.py:477\u001b[0m, in \u001b[0;36mIndexableMixin.add_faiss_index\u001b[0;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose)\u001b[0m\n\u001b[1;32m    473\u001b[0m index_name \u001b[39m=\u001b[39m index_name \u001b[39mif\u001b[39;00m index_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m column\n\u001b[1;32m    474\u001b[0m faiss_index \u001b[39m=\u001b[39m FaissIndex(\n\u001b[1;32m    475\u001b[0m     device\u001b[39m=\u001b[39mdevice, string_factory\u001b[39m=\u001b[39mstring_factory, metric_type\u001b[39m=\u001b[39mmetric_type, custom_index\u001b[39m=\u001b[39mcustom_index\n\u001b[1;32m    476\u001b[0m )\n\u001b[0;32m--> 477\u001b[0m faiss_index\u001b[39m.\u001b[39;49madd_vectors(\n\u001b[1;32m    478\u001b[0m     \u001b[39mself\u001b[39;49m, column\u001b[39m=\u001b[39;49mcolumn, batch_size\u001b[39m=\u001b[39;49mbatch_size, train_size\u001b[39m=\u001b[39;49mtrain_size, faiss_verbose\u001b[39m=\u001b[39;49mfaiss_verbose\n\u001b[1;32m    479\u001b[0m )\n\u001b[1;32m    480\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indexes[index_name] \u001b[39m=\u001b[39m faiss_index\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/datasets/search.py:296\u001b[0m, in \u001b[0;36mFaissIndex.add_vectors\u001b[0;34m(self, vectors, column, batch_size, train_size, faiss_verbose)\u001b[0m\n\u001b[1;32m    294\u001b[0m     train_vecs \u001b[39m=\u001b[39m vectors[:train_size] \u001b[39mif\u001b[39;00m column \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m vectors[:train_size][column]\n\u001b[1;32m    295\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining the index with the first \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_vecs)\u001b[39m}\u001b[39;00m\u001b[39m vectors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 296\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfaiss_index\u001b[39m.\u001b[39;49mtrain(train_vecs)\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mIgnored the training step of the faiss index as `train_size` is None.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/faiss/__init__.py:280\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_train\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    278\u001b[0m n, d \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m    279\u001b[0m \u001b[39massert\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md\n\u001b[0;32m--> 280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_c(n, swig_ptr(x))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/faiss/swigfaiss.py:8779\u001b[0m, in \u001b[0;36mswig_ptr\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   8778\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mswig_ptr\u001b[39m(a):\n\u001b[0;32m-> 8779\u001b[0m     \u001b[39mreturn\u001b[39;00m _swigfaiss\u001b[39m.\u001b[39;49mswig_ptr(a)\n",
      "\u001b[0;31mValueError\u001b[0m: did not recognize array type"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "\n",
    "if 'corpus_size' not in locals():\n",
    "    corpus_size = 1000000\n",
    "\n",
    "if embeddings_ds is None:\n",
    "    print('load dataset from file')\n",
    "    embeddings_ds = load_from_disk('mock_dataset')\n",
    "\n",
    "string_factory = f\"IVF{int(4*sqrt(corpus_size))},Flat\"\n",
    "# string_factory = f\"IVF65536_HNSW32{int(4*sqrt(corpus_size))},Flat\"\n",
    "embeddings_ds.add_faiss_index(column='embedding', string_factory=string_factory, train_size=corpus_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ds.save_faiss_index('embedding', 'mock_dataset/embedding_index.faiss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing result from mock_dataset/state.json to workspaceblobstore\n",
      "Writing result from mock_dataset/dataset_info.json to workspaceblobstore\n",
      "Writing result from mock_dataset/embedding_index.faiss to workspaceblobstore\n",
      "Writing result from mock_dataset/dataset.arrow to workspaceblobstore\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "\n",
    "def write_to_blob(workspace, local_path, upload_path):\n",
    "    datastore = workspace.get_default_datastore()\n",
    "    \n",
    "    account_name = datastore.account_name\n",
    "    account_key = datastore.account_key\n",
    "\n",
    "    connection_string = f'DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net'\n",
    "\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(datastore.container_name)\n",
    "\n",
    "    upload_file = open(local_path, 'rb')\n",
    "    try:\n",
    "        print(f\"Writing result from {local_path} to {datastore.name}\")\n",
    "        container_client.upload_blob(name=upload_path, data=upload_file)\n",
    "    except ResourceExistsError:\n",
    "        print('file name already used')\n",
    "\n",
    "\n",
    "files = glob(\"mock_dataset/*\")\n",
    "for f in files:\n",
    "    write_to_blob(workspace=ws, local_path=f, upload_path=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in dataset with faiss index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download: IVF1922_Flat.faiss\n",
      "Download: dataset.arrow\n",
      "Download: dataset_info.json\n",
      "Download: state.json\n",
      "Dataset({\n",
      "    features: ['_id', 'doc_id', 'title', 'paragraph_id', 'text', 'chunked', 'embeddings'],\n",
      "    num_rows: 230885\n",
      "})\n",
      "<faiss.swigfaiss.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7f7d8c46e030> >\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "from packages.azureml_functions import get_ws\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "\n",
    "ws = get_ws()\n",
    "datastore = ws.get_default_datastore()\n",
    "account_name = datastore.account_name\n",
    "account_key = datastore.account_key\n",
    "connection_string = f'DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net'\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "container_client = blob_service_client.get_container_client('azureml-blobstore-04f263a3-f5c3-4204-9686-0f886c746b64')\n",
    "blob_gen = container_client.list_blobs(name_starts_with=\"corpus_ds_with_embedding/corpus_2021-2021_improved/\")\n",
    "\n",
    "for b in blob_gen:\n",
    "    # step 1: Download the file to local storage\n",
    "    file_name = b.name.rsplit('/',1)[-1]\n",
    "    print(\"Download:\", file_name)\n",
    "\n",
    "    location = \"corpus_embeddings\"\n",
    "    os.makedirs(location, exist_ok=True)\n",
    "    target_path = os.path.join(location, file_name)\n",
    "    with open(target_path, 'wb') as f:\n",
    "        f.write(container_client.download_blob(b.name).readall())  \n",
    "\n",
    "\n",
    "ds = load_from_disk('corpus_embeddings/')\n",
    "ds.load_faiss_index('embedding', \"corpus_embeddings/IVF1922_Flat.faiss\")\n",
    "\n",
    "print(ds)\n",
    "print(ds.get_index('embedding').faiss_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['When do I retire?']\n",
    "\n",
    "query_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "scores, samples = ds.get_nearest_examples('embedding', query_embedding.cpu().detach().numpy(), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_id', 'doc_id', 'title', 'paragraph_id', 'text', 'chunked', 'embeddings'])\n",
      "____________________________________________________________________________________________________\n",
      "[0.85465294 0.90902317 0.91939294 0.9937544  0.9987192 ]\n",
      "[ 0.34544706  3.4341395  -2.6393273  -4.3905954  -1.6300539 ]\n",
      "____________________________________________________________________________________________________\n",
      "[1 0 4 2 3]\n",
      "[167484, 167377, 167777, 67919, 167802]\n",
      "[167377, 167484, 167802, 167777, 67919]\n"
     ]
    }
   ],
   "source": [
    "print(samples.keys())\n",
    "\n",
    "cross_input = list(map(lambda x: query + [x], samples['text']))\n",
    "print('_'*100)\n",
    "output = cross_encoder.predict(cross_input, convert_to_numpy=True)\n",
    "\n",
    "print(scores)\n",
    "print(output)\n",
    "indices = output.argsort()[::-1]\n",
    "\n",
    "def select(indexs, items):\n",
    "    return [items[i] for i in indexs]\n",
    "\n",
    "res_dict = {k: select(indices, v) for k,v in samples.items()}\n",
    "\n",
    "print('_'*100)\n",
    "print(indices)\n",
    "print(samples['_id'])\n",
    "print(res_dict['_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
